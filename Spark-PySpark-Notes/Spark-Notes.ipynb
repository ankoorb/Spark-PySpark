{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#66c2a4></font>\n",
    "<font color=#6baed6></font>\n",
    "<font color=#ef3b2c></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Big Data: <font color=#66c2a4>Volume, Velocity, Variety</font>\n",
    "- ETL - Extraction - Transformation - Loading\n",
    "- SQL - Concept of de-normalization - else problem of consistency\n",
    "- NoSQL - (1) Key-value store; (2) Columnar database; (3) Document Store (e.g. JSON, BSON, XML, PDF); (4) Graph database\n",
    "- Hadoop - HDFS (partition data), Distributed computing - Fault tolerance if machine fails, MapReduce (processing data), YARN - Spark will replace Hadoop\n",
    "- Spark \n",
    "    - NoSQL database has limited function of analytics\n",
    "    - Hadoop MapReduce: slow and complex\n",
    "    - Integrated wit\n",
    "    \n",
    "- Spark: Cluster computing platform\n",
    "    - Provide simple way to parallelize applications aross clusters\n",
    "    - Hides complexity of distributed systems programming, fault tolerence - you do not have to deal with this\n",
    "    - Computational engine scheduling, distribution\n",
    "    \n",
    "- Tachyon - Sparks technology to store data\n",
    "- Driver Program (managees jobs and distributes work), Worker Node (bunch of machines)\n",
    "- Spark is one of the most active Apache project (top level also)\n",
    "- Why Spark is hot? \n",
    "    - <font color=#6baed6>Equivalent to more than  dozen of specialized systems</font>\n",
    "    - Interactive, batch, and real-time within single framework\n",
    "    - Better fault tolerence\n",
    "    - Less code\n",
    "    - Support multiple languages\n",
    "    - In memory fast computation\n",
    "    - High level abstraction\n",
    "    \n",
    "- Why Spark is fast?\n",
    "    - In memory, much faster\n",
    "        - Disk access\n",
    "        - Network flow\n",
    "    - Less code\n",
    "    - Iterative algorithm benefit\n",
    "    - 10 to 100 x faster\n",
    "    \n",
    "- Resilient Distributed Dataset (RDD)\n",
    "- Spark Applications:\n",
    "    - Transformations\n",
    "    - Action \n",
    "    - Accumulators\n",
    "    \n",
    "- User code defines a DAG (Directed Acyclic Graph) of RDDs\n",
    "- Spark web UC: http://localhost/4040\n",
    "- Skew: Some machines finish work fast, idling so machines can be reassigned\n",
    "\n",
    "- Spark SQL\n",
    "- Spark MLlib\n",
    "- \n",
    "\n",
    "- Understanding distributed computing vs single machine computing \n",
    "\n",
    "- Virtual Machine is Virtual Box (Oracle)\n",
    "- BigData University (IBM) virtual machine \n",
    "\n",
    "- junhuicai8@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Spark Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)  \n",
    "[PySpark API Documentation](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "vagrant up\n",
    "vagrant halt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Apachie Spark is a *cluster computing platform*. \n",
    "\n",
    "Spark contains is a `computational engine` that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines or a computing cluster.\n",
    "\n",
    "---\n",
    "![spark-components](./images/stack.png)\n",
    "\n",
    "**Spark Core** - Contains basic functionaliy of Spark (scheduling, memory management, fault recovery, interacting with storage systems, etc.) and API that defines `resilient distributed datasets (RDDS)`\n",
    "\n",
    "**Spark SQL** - Spark's package for working with structured data. Allows querying data vis `SQL`, `HQL`\n",
    "\n",
    "**Spark Streaming** - Spark components that enables processing of live streams of data.\n",
    "\n",
    "**MLlib** - Machine learning library. Algorithms for classification, regression, clustering, and collaborative filtering.\n",
    "\n",
    "**GraphX** - Library for manipulating graphs and performing graph-parallel computations.\n",
    "\n",
    "---\n",
    "\n",
    "In Spark, computations are expressed through operations on distributed collections (`resilient distributed datasets`) that are automatically parallelized across the cluster. RDDs are Spark's fundamental abstraction for distributed data and computation.\n",
    "\n",
    "---\n",
    "Every Spark application consists of a `driver program` that launches various parallel operations on a cluster. The driver program contains an application's main function and defines distributed datasets on the cluster, then applies operations to them. \n",
    "\n",
    "Driver programs access Spark through a `SparkContext` object, which represents a connection to a computing cluster. SparkContext is used to build RDDs and various operations can be performed on RDDs.\n",
    "\n",
    "![distributed-execution](./images/distributed.png)\n",
    "\n",
    "To run operations, driver programs typically manage a large number of nodes called `excutors`. Spark automatically takes a function and ships it to the executor nodes.\n",
    "\n",
    "In Spark all work is expressed as either creating new RDDs, transforming new RDDs, or calling operations on RDDs to compute a result. \n",
    "\n",
    "---\n",
    "**Resilient Distributed Dataset** - An RDD is simply an immutable distributed collection of objects. Each RDD is split into multiple `partitions`, which may be computed on different nodes of the cluster. RDD can contain any type of Python, Java, or Scala objects, including user defined classes.\n",
    "\n",
    "> Think of each RDD as consisting of instructions on how to compute the data that has been built up through transformations.\n",
    "\n",
    "\n",
    "RDDs can be created in two ways by:\n",
    "- Loading an external dataset\n",
    "- Distributing a collection of objects (e.g., a list or set) in their driver programs, i.e. Parallelizing a collection in your driver program.\n",
    "\n",
    "Once created RDDs offer two types of operations:\n",
    "\n",
    "- **Transformations** are operations on RDDs that return a new RDD. Most transformations are *element-wise*. As new RDDs are derived from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the `lineage graph`\n",
    "\n",
    "![lineage-graph](./images/lineage.png)\n",
    "\n",
    "- **Actions** are operations that return a result to the driver program  or write it to a storage and kick off a computation. i.e. compute a result based on an RDD, and either return it to the driver program or save it to an external storage system. \n",
    "\n",
    "> Confused whether a given function is a transformation or an action: look at its return type: `Transformations return RDDs`, whereas actions return some other data type.\n",
    "\n",
    "Transformations and actions are different because of the way spark computes RDDs. Although a new RDD can be defined any time, Spark computes them only in a `lazy` fashion, i.e. the first time they are used in an action. `Lazy` fashion computation reduces wastage of storage space as Spark sees the whole chain of transformations, it computes just the data needed for its result. Transformations on RDDs are lazily evaluated, meaning that Spark will not begin to execute until it sees an action. Lazy evaluation means that when a transformation is called on an RDD, the operation is not immediately performed. Instead, Spark internally records metadata to indicate that this operation has been requested. Spark uses lazy evaluation to reduce the number of passes it has to take over data by grouping operatons together.\n",
    "\n",
    "RDDs by default are `recomputed` from scratch each time an action is run on them. To avoid this inefficiency and to reuse an RDD in multiple actions ask Spark to `persist` it. When Spark `persists` an RDD, the nodes that compute the RDD store their partitions. If a node that has data persisted on it fails, Spark will recompute the lost partitions of the data when needed. \n",
    "\n",
    "In Python, the data that persist stores is always searialized, so the default is instead stored in the Java Virtual Machine as pickled objects.\n",
    "\n",
    "If caching of too much data to fit in memory is attempted, Spark will automatically evict old partitions using a `Least Recently Used (LRU)` cache policy.\n",
    "\n",
    "> The ability to always recompute an RDD is actually why RDD's are called `resilient`. When a machine holding RDD data fails, Spark uses this ability to recomputed the missing partitions.\n",
    "\n",
    "---\n",
    "Every Spark program and shell session will work as follows:\n",
    "\n",
    "1. Create some input RDDs from external data\n",
    "2. Transform input RDDs to define new RDDs using `transformations`\n",
    "3. Ask Spark to `persist` any intermediated RDDs that will need to be reused\n",
    "4. Launch `actions` to start a parallel computation, which is then optimized and executed by Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDD\n",
    "\n",
    "- `sc.textFile(<file>)` - Loading a text file\n",
    "- `sc.parallelize(<collecion>)` - Simplest way to create RDDs is to take an existing collection and pass it to this method\n",
    "- `RDD.persist()` - Asking Spark to persist an RDD\n",
    "- `RDD.unpersist()` - Manually remove RDDs from the cache\n",
    "\n",
    "## Transformations\n",
    "\n",
    "- `RDD.map(<function>)` - Transformation takes in a function and applies to each element in the RDD with the result of the function being the new value of each element in the resulting RDD\n",
    "- `RDD.filter(<function>)` - Transformation takes in a function and returns an RDD that only has elements that pass the filter funtion.\n",
    "![map-filter](./images/trans-1.png)\n",
    "- `RDD.flatMap(<function>)` - Transformation takes in a function and applies to each element in the RDD and returns an iterator with return values, i.e. returns multiple output elements for each input element\n",
    "![flatMap](./images/flatmap.png)\n",
    "\n",
    "#### Pseudo Set Operations (RDDs are not sets)\n",
    "- `RDD.distinct()` - Transformation returns a new RDD with only distinct or unique items (expensive!)\n",
    "- `RDD.union(other)` - Set operation returns an RDD consisting of the data from both sources\n",
    "- `RDD.intersection(other)` - Set operaton returns only elements in both RDDs\n",
    "- `RDD.subtract(other)` - Set operation takes in another RDD and returns an RDD that has only values present in the first RDD and not the second\n",
    "![](./images/setop.png)\n",
    "- `RDD.cartesian(other)` - Transformation returns all possible pairs of `(a, b)` where `a` is in the source RDD and `b` is in the other RDD (very expensive!)\n",
    "![](./images/cart.png)\n",
    "- `RDD.sample(withReplacement, fraction, [seed])` - Sample and RDD with or without replacement\n",
    "\n",
    "## Actions\n",
    "- `RDD.reduce(<function>)` - Action takes a function that operates on *two elements of the type in an RDD* and return *a new element of the same type*. Simple example: $+$ used to sum an RDD. \n",
    "```python \n",
    "RDD.reduce(lambda x, y: x + y)\n",
    "```\n",
    "- `RDD.fold()` - \n",
    "- `RDD.aggregate(<function>)` - Avoids the constraint of having the return be the same type as the RDD that is being operated on. Need to supply an initial zero value () of the type that is intended to be returned. Then supply a function to combine the elements from RDD with accumulator. Finally supply a second function to merge two accumulator, given that each node accumulates its own results locally.\n",
    "```python\n",
    "RDD.aggregate((0, 0),\n",
    "(lambda k, v: (k[0] + v, k[1] + 1),\n",
    "(lambda k1, k2: (k1[0] + k2[0], k1[1] + k2[1])))) # Summing count\n",
    "```\n",
    "- `RDD.collect()` - Action returns entire RDD's data to driver program. It suffers from the restriction that all your data must fit on a single machine, as it all needs to be copied to the driver.\n",
    "- `RDD.take(n)` - Action returns `n` elements from the RDD and attempts to minimize the number of partitions it accesses, so it may represent a biased collection. These operations do not return the elements in the expected order.\n",
    "- `RDD.top(n)` - Action returns `n` top elements from an RDD\n",
    "- `RDD.takeSample(withReplacement, n, seed)` - Action returns sample of RDD either with or without replacement\n",
    "- `RDD.foreach(<function>)` - Perform computation on each element in the RDD without bringing it back locally\n",
    "- `RDD.count()` - Returns a count of the elements in an RDD\n",
    "- `RDD.countByValue()` - Returns a map of each unique value to its count, i.e. Number of times each element occurs in the RDD\n",
    "- `RDD.takeOrdered(n, ordering)` - Return `n` elements based on provided ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key/Value RDDs are commonly used to perform aggregations. Often need to do some initial `ETl (Extract-Transform-Load)` to get data into Key/Value format\n",
    "\n",
    "RDDs containing Key/Value pairs are called `Pair RDDs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# Extract a Field from an RDD Row object. NOTE: Prediction results are RDDs with Row objects\n",
    "RDD.map(lambda i: i.Field_Name)\n",
    "\n",
    "# Minimum value in an RDD\n",
    "RDD.min() \n",
    "\n",
    "# Maximum value in an RDD\n",
    "RDD.max() \n",
    "\n",
    "# Unique items in an RDD\n",
    "RDD.distinct()\n",
    "\n",
    "# Count number of items in an RDD\n",
    "RDD.count()\n",
    "\n",
    "# Sum RDD values\n",
    "RDD.sum()\n",
    "\n",
    "# Sum elements of RDD\n",
    "RDD.reduce(lambda x, y: x + y)\n",
    "\n",
    "# For counting purposes use lambda to get tuple: (key, 1)\n",
    "RDD.map(lambda key: (key, 1)) \n",
    "\n",
    "# Check intermediate result (Most useful action!!!)\n",
    "RDD.take(n) # n = int\n",
    "\n",
    "# Persist an RDD with default storage level\n",
    "RDD.cache()\n",
    "\n",
    "# Test if an RDD is Cached\n",
    "RDD.is_cached\n",
    "\n",
    "# Concatinate 2 RDDS (vertically), i.e. Produce an RDD containing elements from both RDDS\n",
    "RDD.union(otherRDD)\n",
    "\n",
    "# Produce an RDD containing only elements found in both RDDs\n",
    "RDD.intersection(otherRDD)\n",
    "\n",
    "# Combine 2 RDDs by taking cartesian product: RDD = [a], otherRDD = [b]. Cartesian Product = [('a', 'b'), ('a', 'c')]\n",
    "RDD.cartesian(otherRDD)\n",
    "\n",
    "## Pair RDDs i.e. RDDs with tuples (key, count)\n",
    "# Summing RDDs with tuples (key, 1)\n",
    "pairRDD.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Filtering RDD with tuples (key, count)\n",
    "pairRDD.filter(lambda i: i[1] > value)\n",
    "pairRDD.filter(lambda i: i[0] == string)\n",
    "\n",
    "# Extract Keys from RDDs with tuples (key, count) - 1\n",
    "pairRDD.map(lambda i: i[0]) \n",
    "\n",
    "# Extract Keys from RDDs with tuples (key, count) - 2\n",
    "pairRDD.map(lambda (x, y): x) \n",
    "\n",
    "# Extract counts from RDDs with tuples (key, count)\n",
    "pairRDD.map(lambda (x, y): y) \n",
    "\n",
    "# flatMap example: [(a, [1, 2]), (b, [3, 2, 4])] to [1, 2, 3, 2, 4]\n",
    "RDD.flatMap(lambda i: i[1])\n",
    "\n",
    "## Access Iterable Object elements by converting it to list\n",
    "# Example: [('a', 12), ('b', 10), ('a', 6), ('c', 3), ('b', 8), ('a', 2), ('b', 5)]\n",
    "# pairRDD.groupByKey().collect() returns [('a', <iterable_obj_1>), ('b', <iterable_obj_2>), ('c', <iterable_obj_3)]\n",
    "# Converting Iterable Object to a list\n",
    "pairRDD.groupByKey().map(lambda i: (i[0], list(i[1])))\n",
    "\n",
    "# Extracting top 'n' (key, value) pairs - as per keys or values\n",
    "pairRDD.takeOrdered(n, lambda i: i[1]) # Ascending as per values\n",
    "pairRDD.takeOrdered(n, lambda i: -1 * i[1]) # Descending as per values - lambda function multiplies the count by -1\n",
    "\n",
    "# Extracting top 'n' (key, value) pairs - as per key function\n",
    "pairRDD.takeOrdered(n, key = key_function)\n",
    "\n",
    "# Convert a pair RDD to a list\n",
    "pairRDD.collect()\n",
    "\n",
    "# Convert a pair RDD to a dict\n",
    "pairRDD.collectAsMap()\n",
    "\n",
    "# Sort a pair RDD by Key\n",
    "pairRDD.sortByKey(ascending = True)\n",
    "\n",
    "# Sort a pair RDD\n",
    "pairRDD.sortBy(key_function, ascending = True, numPartitions = None)\n",
    "\n",
    "# Group By Key operation on Pair RDDs\n",
    "pairRDD.groupByKey() # Generates a pair RDD of type (key, iterator)\n",
    "groupedPairRDD.map(lambda i: (i[0], sum(i[1]))) # Count\n",
    "groupedPairRDD.map(lambda i: (i[0], len(set(i[1])))) # Unique value count\n",
    "groupedPairRDD.map(lambda i: (i[0], float(len(i[1]))/len(set(i[1])))) # Average\n",
    "\n",
    "## Broadcast Variables\n",
    "# Creating Broadcast Variables\n",
    "broadcastVariable = sc.broadcast(v) # v can be: list, dict, tuple\n",
    "\n",
    "# Accessing Broadcast Variables values\n",
    "broadcastVariable.value\n",
    "\n",
    "# Join 2 pair RDDs RDD => (K, V) and otherRDD => (K, W) to get new pair RDD (K, (V, W))\n",
    "pairRDD.join(otherPairRDD)\n",
    "\n",
    "\n",
    "# Return each (key, value) pair in an RDD that has no pair with matching key in other RDD\n",
    "pairRDD.subtractByKey(otherPairRDD)\n",
    "\n",
    "## Extracting value from a tuple inside a tuple, i.e. Extract K and W from (K, (V, W))\n",
    "pairRDD.map(lambda (K, (V, W): (K, W))\n",
    "\n",
    "# Or\n",
    "pairRDD.map(lambda (K, U): (K, U[0])) # Here U = (V, W)\n",
    "\n",
    "## Repartition when using a \"gzip\" file: a single gzipped file cannot be loaded in parallel by multiple tasks, so Spark will load it with 1 task and thus give you an RDD with 1 partition. A gzipped file is not splittable, so Spark will always use 1 task to read the file\n",
    "RDD = sc.textFile(gzipFileName).repartition(num_Partitions) \n",
    "\n",
    "## Lab-3 Spark: \"Accumulators\", \"Left Join\"\n",
    "\n",
    "## Whenever we examine only a subset of a large dataset, there is the potential that the result will depend on the order we perform operations, such as joins, or how the data is partitioned across the workers. What we want to guarantee is that we always see the same results for a subset, independent of how we manipulate or store the data. We can do that by sorting before we examine a subset. You might think that the most obvious choice when dealing with an RDD of tuples would be to use the sortByKey() method. However this choice is problematic, as we can still end up with different results if the key is not unique. A better technique is to sort the RDD by both the key and value, which we can do by combining the key and value into a single string and then sorting on that string.\n",
    "\n",
    "## Splitting an RDD into Training, Validation and Test sets\n",
    "RDD.randomSplit([weights], seed_number)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate By Key\n",
    "\n",
    "```python\n",
    "RDD = sc.parallelize([('a', 2), ('b', 5), ('a', 3), ('b', 1), ('c', 4), ('a', 3), ('c', 7)])\n",
    "\n",
    "aggregateByKeyRDD = RDD.aggregateByKey((0, 0), \n",
    "                                        lambda x, y: (x[0] + y,    x[1] + 1),\n",
    "                                        lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "print aggRDD.take(3)\n",
    "\n",
    "> [('a', (8, 3)), ('c', (11, 2)), ('b', (6, 2))]\n",
    "```\n",
    "\n",
    "1. First lambda expression: <font color=steelblue>**lambda x, y: (x[0] + y,    x[1] + 1)**</font> is for `Within-Partition Reduction Step`, where:\n",
    "    - **x** - is a Tuple that holds: **(runningSum, runningCount)**\n",
    "    - **y** - is a Scalar that holds: the **next value**\n",
    "\n",
    "2. Second lambda expression: <font color=steelblue>**lambda x, y: (x[0] + y[0], x[1] + y[1])**</font> is for `Cross-Partition Reduction Step`, where:\n",
    "    - **x** - is a Tuple that holds: **(runningSum, runningCount)**\n",
    "    - **y** - is a Tuple that holds: **(nextPartitionsSum, nextPartitionsCount)**\n",
    "    \n",
    "```python\n",
    "RDD.aggregateByKey(()\n",
    "                 lambda function for Within Partition Reduction Step - Accumulator, value,\n",
    "                 lambda function for Cross Partition Reduction Step - Accumulator-1, Accumulator-2)\n",
    "```\n",
    "\n",
    "\n",
    "### Combine By Key\n",
    "\n",
    "[Reference](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)\n",
    "\n",
    "**combineByKey** Method requires 3 lambda functions:\n",
    "   - *createCombiner* - First aggregation step for each key. The argument of this function corresponds to the `value` in a `key-value` pair\n",
    "   - *mergeValue* - Tells `combineByKey` what to do when a combiner is given a new value\n",
    "   - *mergeCombiner* - Tells `combineByKey` how to merge two combiners\n",
    "\n",
    "```python\n",
    "RDD.combineByKey(createCombiner\n",
    "                 mergeValue,\n",
    "                 mergeCombiners)\n",
    "                                \n",
    "                 \n",
    "RDD = sc.parallelize([('a', 2), ('b', 5), ('a', 3), ('b', 1), ('c', 4), ('a', 3), ('c', 7)])\n",
    "\n",
    "# Result: (key, (sum, count))\n",
    "combineByKeyRDD = RDD.combineByKey(lambda value: (value, 1),\n",
    "                                   lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                                   lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "print combineByKeyRDD.take(3)\n",
    "\n",
    "> [('a', (8, 3)), ('c', (11, 2)), ('b', (6, 2))]\n",
    "```\n",
    "> #### Computing Sum and Count using `combineByKey`\n",
    "> **`Create Combiner Function: `** <font color=steelblue>**lambda** value: (value, 1)</font> - This function is the first aggregation step for each key. The argument of this function corresponds to the **`value`** in a **`key-value`** pair. For sum and count: create **combiner** to be a tuple in the form of **(sum, count)**. The very first step in this aggregation is then **(value, 1)**, where **value** is the first RDD value that **`combineByKey`** comes across and **1** initializes the count\n",
    "\n",
    "---\n",
    "> **`Merge Value Function: `** <font color=steelblue>**lambda** x, value: (x[0] + value, x[1] + 1)</font> - This function tells **`combineByKey`** what to do when a **combiner** is given a **new value**. The arguments to this function are a **`combiner`** and a **`new value`**. For sum and count the **combiner** is defined as a tuple in the form of **(sum, count)**. The **`new value`** is merged by adding it to the first element of the tuple while incrementing **1** to the second element of the tuple\n",
    "\n",
    "---\n",
    "> **`Merge Combiners Function: `** <font color=steelblue>**lambda** x, y: (x[0] + y[0], x[1] + y[1])</font> - This function tells **`combineByKey`** how to merge two **combiners**. For sum and count the **combiners** are defined as a tuples in the form of **(sum, count)**. The combiners are merged by adding the first and last elements together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "\n",
    "## Create an array\n",
    "np.array([...])\n",
    "\n",
    "## Scalar multiplication with an ndarray use: *\n",
    "value * np.array([...])\n",
    "\n",
    "## Element-wise multiplication\n",
    "x * y\n",
    "\n",
    "## Dot product\n",
    "np.dot(x, y)\n",
    "\n",
    "# or\n",
    "x.dot(y)\n",
    "\n",
    "## Generate a matrix\n",
    "np.matrix([[...], [...]])\n",
    "\n",
    "## Transpose of a matrix\n",
    "x.T\n",
    "\n",
    "## Inverting a square matrix. Note: Square matrices are not guaranteed to have an inverse.\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Note: inv(x) * x = I\n",
    "inv(x)\n",
    "\n",
    "## Combining ndarrays\n",
    "# Combine arrays column-wise\n",
    "np.hstack((x, y))\n",
    "\n",
    "# Combine arrays row-wise\n",
    "np.vstack((x, y))\n",
    "\n",
    "## PySpark's DenseVector (DenseVector is used to store arrays of values for use in PySpark.  DenseVector actually stores values in a NumPy array and delegates calculations to that object. A new DenseVector is created using DenseVector() and passing in an NumPy array or a Python list. Note: DenseVector stores all values as np.float64 and DenseVector objects exist locally and are not inherently distributed.  DenseVector objects can be used in the distributed setting by either passing functions that contain them to resilient distributed dataset (RDD) transformations or by distributing them directly as RDDs.)\n",
    "\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "\n",
    "DenseVector.dot()\n",
    "\n",
    "## Functional Programming: In functional programming, you will often pass functions to other functions as parameters, and lambda can be used to reduce the amount of code necessary and to make the code more readable. Some commonly used functions in functional programming are map, filter, and reduce. Map transforms a series of elements by applying a function individually to each element in the series. It then returns the series of transformed elements. Filter also applies a function individually to each element in a series; however, with filter, this function evaluates to True or False and only elements that evaluate to True are retained. Finally, reduce operates on pairs of elements in a series. It applies a function that takes in two values and returns a single value.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Broadcast\n",
    "- Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
